{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 1: What is the role of filters and feature maps in Convolutional Neural Network (CNN)?**\n",
        "\n",
        "Filters (also known as kernels) and feature maps are fundamental components of a Convolutional Neural Network (CNN). Here's a breakdown of their roles:\n",
        "\n",
        "**Filters (Kernels):**\n",
        "\n",
        "**- Feature Detection:** Filters are small matrices of numbers that slide across the input image (or the output of a previous layer). Each filter is designed to detect a specific type of feature, such as edges (horizontal, vertical, diagonal), corners, textures, or more complex patterns. For example, one filter might activate strongly when it encounters a vertical edge, while another might activate for a horizontal edge.\n",
        "\n",
        "**- Weight Sharing:** A key characteristic of CNNs is that the same filter is applied across the entire input image. This weight sharing significantly reduces the number of parameters in the network, making it more efficient and helping it generalize better to new, unseen data.\n",
        "\n",
        "**- Learned Representations:** During the training process, the values within these filters are learned automatically by the network to best identify relevant features for the given task (e.g., image classification).\n",
        "\n",
        "**Feature Maps (Activation Maps):**\n",
        "\n",
        "**- Output of Convolution:** When a filter convolves (slides over) an input image or feature map, it performs element-wise multiplication and summation. The result of this operation at each position creates a single value in the output.\n",
        "\n",
        "**- Spatial Representation of Features:** The collection of these output values, generated by a single filter across the entire input, forms a feature map. Each feature map represents the presence and strength of the specific feature that its corresponding filter is designed to detect, across different spatial locations in the input.\n",
        "\n",
        "**- Hierarchical Learning:** As data passes through multiple convolutional layers in a CNN, early layers detect simple features (like edges). Subsequent layers combine these simple features to detect more complex, abstract features (like eyes, noses, or entire objects). Each layer's output is a set of feature maps that serve as the input for the next layer, building a hierarchical representation of the input data."
      ],
      "metadata": {
        "id": "A4Y94Luv87TY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?**\n",
        "\n",
        "Padding and stride are two important concepts in Convolutional Neural Networks (CNNs) that significantly influence the dimensions of the output feature maps. Let's break them down:\n",
        "\n",
        "**Padding:**\n",
        "\n",
        "**- Concept:** Padding involves adding extra pixels (typically zeros) around the border of the input image or feature map before applying the convolution operation. This is also known as 'zero-padding' when the added pixel values are zero.\n",
        "\n",
        "**- Purpose:** The primary purposes of padding are:\n",
        "\n",
        "**1.Preserving Spatial Dimensions:** Without padding, the output feature map shrinks with each convolutional layer, especially at the borders, as the filter cannot be centered over border pixels. Padding helps maintain the spatial dimensions of the input, allowing the output feature map to have the same or a similar size as the input.\n",
        "\n",
        "**2.Retaining Information:** Pixels at the borders of an image are processed fewer times than central pixels during convolution. Padding ensures that border pixels contribute more to the output, preventing loss of information from the edges of the input.\n",
        "\n",
        "**3.Allowing Deeper Networks:** By preventing excessive reduction in feature map size, padding enables the construction of deeper CNNs without losing all spatial information too quickly.\n",
        "\n",
        "**Types:** Common types include:\n",
        "\n",
        "**- 'Valid' (No Padding):** No padding is added. The output feature map will be smaller than the input.\n",
        "\n",
        "**- 'Same' (Zero Padding):** Padding is added such that the output feature map has the same spatial dimensions as the input feature map, assuming a stride of 1.\n",
        "\n",
        "**Stride:**\n",
        "\n",
        "**- Concept:** Stride defines the number of pixels by which the convolution filter shifts across the input image or feature map. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means it moves two pixels at a time, and so on.\n",
        "\n",
        "**- Purpose:** The main purposes of stride are:\n",
        "\n",
        "**1.Downsampling/Dimensionality Reduction:** A stride greater than 1 effectively downsamples the input feature map. This reduces the spatial dimensions, which helps in reducing computational cost and controlling overfitting by extracting a more abstract representation of the features.\n",
        "\n",
        "**2.Increasing Receptive Field:** By skipping pixels, a larger stride allows subsequent layers to have a larger 'receptive field' (the area of the original input image that a pixel in the feature map corresponds to) without increasing the filter size.\n",
        "\n",
        "**How they affect the output dimensions of feature maps:**\n",
        "\n",
        "The output dimension of a feature map (let's say for a square input and filter for simplicity) can be calculated using the following formula:\n",
        "\n",
        "Output Dimension = [(Input Dimension - Filter Dimension + 2 * Padding) / Stride] + 1\n",
        "\n",
        "Let's break down the components:\n",
        "\n",
        "**- Input Dimension (W):** The height or width of the input feature map.\n",
        "\n",
        "**- Filter Dimension (F):** The height or width of the convolutional filter/kernel.\n",
        "\n",
        "**- Padding (P):** The number of pixels added to each side of the input (if padding is symmetric).\n",
        "\n",
        "**- Stride (S):** The number of pixels the filter shifts at each step.\n",
        "\n",
        "**Impact of each:**\n",
        "\n",
        "Padding (P): Increasing padding increases the output dimensions. If P=0, the output will be smaller than the input (for S=1, F>1). If P is chosen correctly (e.g., for 'same' padding), the output dimension can be kept equal to the input dimension.\n",
        "Stride (S): Increasing stride decreases the output dimensions. A larger stride means fewer steps the filter takes across the input, resulting in a smaller output feature map. A stride of 1 maintains the highest spatial resolution (given appropriate padding), while a stride of 2 or more reduces it.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Consider an input image of size (10, 10), a filter of size (3, 3):\n",
        "\n",
        "1.No Padding, Stride 1 (P=0, S=1): Output = [(10 - 3 + 2 * 0) / 1] + 1 = 7 + 1 = 8. Output size: (8, 8).\n",
        "\n",
        "2.Padding 1, Stride 1 (P=1, S=1) (often results in 'same' size output for odd filter sizes): Output = [(10 - 3 + 2 * 1) / 1] + 1 = (7 + 2) + 1 = 9 + 1 = 10. Output size: (10, 10).\n",
        "\n",
        "3.No Padding, Stride 2 (P=0, S=2): Output = [(10 - 3 + 2 * 0) / 2] + 1 = [7 / 2] + 1 = 3 + 1 = 4. Output size: (4, 4) (Note: result is floored for integer output).\n",
        "\n",
        "In essence, padding helps control the shrinkage of feature maps, while stride controls the downsampling rate, both working together to manage the spatial dimensions and information flow through the CNN."
      ],
      "metadata": {
        "id": "bgooiNsn9pZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 3: Define receptive field in the context of CNNs. Why is it important for deep architectures?**\n",
        "\n",
        "The **receptive field** in a Convolutional Neural Network (CNN) refers to the region in the input space (e.g., the original input image) that a particular neuron in a feature map is 'looking at' or influenced by. In simpler terms, it's the area of the input image that contributes to the computation of a single output feature.\n",
        "\n",
        "Let's break it down:\n",
        "\n",
        "**For the first convolutional layer:** The receptive field of a neuron in the first layer's output feature map is simply the size of the filter itself.\n",
        "\n",
        "**For deeper layers:** As you move deeper into the network, the receptive field of neurons in subsequent layers becomes progressively larger. Each neuron in a deeper layer's feature map effectively aggregates information from a wider area of the previous layer's feature map, which in turn corresponds to an even wider area of the original input image.\n",
        "\n",
        "**Why is it important for deep architectures?**\n",
        "\n",
        "The increasing receptive field in deep CNNs is crucial for several reasons:\n",
        "\n",
        "**1.Hierarchical Feature Extraction:** Early layers with small receptive fields detect local, low-level features (e.g., edges, corners). As the receptive field grows in deeper layers, neurons can combine these low-level features to recognize more complex, abstract, and global patterns (e.g., textures, parts of objects, or even entire objects). This hierarchical understanding is fundamental to how CNNs achieve impressive performance in tasks like object recognition.\n",
        "\n",
        "**2.Contextual Understanding:** A larger receptive field allows the network to incorporate more contextual information when making predictions about a specific part of the image. For instance, to identify a 'face', a neuron needs to see not just an 'eye' or a 'nose' but also their spatial relationship within a broader facial structure.\n",
        "\n",
        "**3.Efficiency:** Instead of using extremely large filters in the first layer to capture global patterns (which would be computationally expensive and require many parameters), deep architectures achieve large receptive fields by stacking multiple smaller filters. This strategy, combined with pooling layers and strides, is far more efficient in terms of parameters and computation.\n",
        "\n",
        "**4.Spatial Invariance/Robustness:** By gradually increasing the receptive field, the network becomes more robust to small variations in the position or scale of features. A neuron in a deep layer might recognize a feature regardless of its exact pixel location, as it's informed by a wider input area.\n",
        "\n",
        "In essence, the receptive field determines the scope of information available to a neuron at each level of abstraction within the network, allowing deep CNNs to learn rich and complex representations from raw input data."
      ],
      "metadata": {
        "id": "PQqHTuziCPbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 4: Discuss how filter size and stride influence the number of parameters in a CNN.**\n",
        "\n",
        "Filter size and stride are critical hyperparameters in Convolutional Neural Networks (CNNs) that significantly influence the number of parameters in the network. Let's break down their impact:\n",
        "\n",
        "**1. Filter Size (Kernel Size):**\n",
        "\n",
        "**- Impact on Parameters:** The filter (or kernel) is a small matrix of weights that slides across the input. The number of parameters contributed by a single convolutional layer is primarily determined by:\n",
        "(Filter Width * Filter Height * Number of Input Channels + 1 (for bias)) * Number of Filters\n",
        "\n",
        "**- Direct Relationship:** As the filter size increases (e.g., from 3x3 to 5x5 to 7x7), the number of weights within each filter increases quadratically. Consequently, the total number of parameters in that convolutional layer increases proportionally.\n",
        "For example, if you have 1 input channel and 32 output filters:\n",
        "\n",
        "A 3x3 filter: (3 * 3 * 1 + 1) * 32 = 10 * 32 = 320 parameters.\n",
        "\n",
        "A 5x5 filter: (5 * 5 * 1 + 1) * 32 = 26 * 32 = 832 parameters.\n",
        "\n",
        "**- Trade-offs:** Larger filters can capture broader features but lead to more parameters, increasing computational cost and the risk of overfitting, especially with limited data. Smaller filters (like 3x3) are often preferred in modern architectures because stacking multiple small filters can achieve the same receptive field as one large filter but with fewer parameters and more non-linearities.\n",
        "\n",
        "**2. Stride:**\n",
        "\n",
        "**- Impact on Parameters:** Stride defines how many pixels the filter moves at each step. Critically, stride itself does NOT directly influence the number of parameters (weights and biases) within the convolutional filters. The weights and biases of a filter are fixed regardless of how many steps it takes across the input.\n",
        "\n",
        "**- Indirect Influence:** While stride doesn't change the number of parameters per se, it affects the size of the output feature maps. A larger stride reduces the spatial dimensions of the output feature map. This reduction has an indirect impact on the number of parameters in subsequent layers (especially fully connected layers, if any, that process the flattened output of convolutional layers) because a smaller feature map means fewer inputs to those subsequent layers. For example, if you have a pooling layer or a fully connected layer following a convolutional layer, a smaller feature map (due to a larger stride in the preceding convolutional layer) will result in fewer parameters in those subsequent layers.\n",
        "\n",
        "**- Computational Cost:** A larger stride also significantly reduces the number of computations required in a convolutional layer because the filter performs fewer operations on the input.\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "**Filter size directly increases the number of parameters** within a convolutional layer. Larger filters mean more weights to learn.\n",
        "\n",
        "**Stride does not directly affect the number of parameters** in a convolutional layer. However, it **indirectly affects the number of parameters in subsequent layers** by altering the spatial dimensions of the feature maps, and it directly impacts the computational cost."
      ],
      "metadata": {
        "id": "wQTOY3B4Fxd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 5: Compare and contrast different CNN-based architectures like LeNet,AlexNet, and VGG in terms of depth, filter sizes, and performance.**\n",
        "\n",
        "Here's a comparison and contrast of LeNet, AlexNet, and VGG, focusing on their depth, filter sizes, and performance:\n",
        "\n",
        "**1. LeNet-5 (1998)**\n",
        "\n",
        "**Context:** One of the earliest and foundational CNNs, primarily designed for handwritten digit recognition (e.g., ZIP codes).\n",
        "\n",
        "**Depth:** Relatively shallow, typically 7 layers (3 convolutional, 2 pooling, 2 fully connected).\n",
        "\n",
        "**Filter Sizes:** Used small 5x5 convolutional filters.\n",
        "\n",
        "**Key Innovations:** Introduced the core concepts of CNNs: convolutional layers, pooling layers, and fully connected layers. Employed shared weights and local receptive fields.\n",
        "**Performance:** Achieved good performance on tasks like MNIST handwritten digit recognition, but limited by today's standards and not scalable to complex image datasets.\n",
        "\n",
        "**2. AlexNet (2012)**\n",
        "\n",
        "**Context:** A groundbreaking architecture that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, significantly outperforming traditional methods. This sparked the deep learning revolution.\n",
        "\n",
        "**Depth:** Deeper than LeNet, typically 8 layers (5 convolutional, 3 fully connected).\n",
        "\n",
        "**Filter Sizes:** Started with a large 11x11 filter in the first convolutional layer, followed by 5x5 and 3x3 filters in subsequent layers.\n",
        "\n",
        "**Key Innovations:** Showcased the power of deeper networks, ReLU activation functions (addressing vanishing gradients), dropout (for regularization), data augmentation, and GPU training (using two GPUs). Its large filter size in the initial layer was a distinctive feature.\n",
        "\n",
        "**Performance:** Achieved a top-5 error rate of 15.3% on ImageNet, a significant leap forward. It demonstrated that deeper CNNs trained on large datasets with GPUs could achieve state-of-the-art results on complex image recognition tasks.\n",
        "\n",
        "**3. VGGNet (2014)**\n",
        "\n",
        "**Context:** Developed by the Visual Geometry Group at Oxford, it was a runner-up in ILSVRC 2014 and further emphasized the importance of network depth.\n",
        "\n",
        "**Depth:** Significantly deeper, with architectures like VGG-16 (16 layers) and VGG-19 (19 layers) being common. It focused on increasing depth by stacking many convolutional layers.\n",
        "\n",
        "**Filter Sizes:** A defining characteristic of VGG is its exclusive use of very small 3x3 convolutional filters throughout the network. It showed that stacking multiple 3x3 filters could achieve the same receptive field as a larger filter (e.g., two 3x3 layers have an effective receptive field of 5x5, and three 3x3 layers have a 7x7 receptive field) but with fewer parameters and more non-linearities.\n",
        "\n",
        "**Key Innovations:** Emphasized the idea of increasing depth with small 3x3 filters as a way to improve performance. Showed that simplicity and uniformity in architecture (repeated blocks of 3x3 convolutions and 2x2 max-pooling) could lead to excellent results.\n",
        "\n",
        "**Performance:** Achieved a top-5 error rate of 7.3% on ImageNet, further pushing the boundaries of accuracy. However, its very deep structure and numerous filters led to a very large number of parameters (e.g., VGG-16 has 138 million parameters), making it computationally expensive and memory-intensive.\n",
        "\n",
        "In Summary:\n",
        "\n",
        "LeNet was a conceptual pioneer, establishing the foundation for CNNs, but was limited in scale and complexity.\n",
        "AlexNet was the breakthrough, demonstrating that deeper CNNs, combined with modern techniques (like ReLU and GPUs), could solve complex image recognition problems effectively.\n",
        "VGG refined the architectural approach by showing the power of extreme depth achieved through uniform small 3x3 filters, further boosting performance but at the cost of significantly increased parameters and computational demands. Its simplicity and effectiveness made it a popular baseline for subsequent research."
      ],
      "metadata": {
        "id": "6AqwOatHIZkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 6: Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.**\n"
      ],
      "metadata": {
        "id": "B5Up8a10TdP2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4362372a"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "90c65d04",
        "outputId": "eb1458d2-b75f-43de-ab42-d0966e839012"
      },
      "source": [
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax') # 10 classes for MNIST digits\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eb25245"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9f068f5",
        "outputId": "ecb38f3c-4606-4712-cd73-3de4783e6869"
      },
      "source": [
        "# Train the model\n",
        "# Assuming x_train_reshaped and y_train_one_hot are already loaded and preprocessed\n",
        "# from previous steps based on the kernel state.\n",
        "history = model.fit(x_train_reshaped, y_train_one_hot, epochs=5, batch_size=128, validation_split=0.1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 111ms/step - accuracy: 0.8554 - loss: 0.4990 - val_accuracy: 0.9752 - val_loss: 0.0822\n",
            "Epoch 2/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 103ms/step - accuracy: 0.9798 - loss: 0.0643 - val_accuracy: 0.9870 - val_loss: 0.0491\n",
            "Epoch 3/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 111ms/step - accuracy: 0.9881 - loss: 0.0384 - val_accuracy: 0.9890 - val_loss: 0.0413\n",
            "Epoch 4/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 106ms/step - accuracy: 0.9911 - loss: 0.0297 - val_accuracy: 0.9900 - val_loss: 0.0362\n",
            "Epoch 5/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 109ms/step - accuracy: 0.9929 - loss: 0.0216 - val_accuracy: 0.9882 - val_loss: 0.0353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b18557a",
        "outputId": "87cf4a97-1ff6-4b94-a4b0-38cb5a91269d"
      },
      "source": [
        "# Evaluate the model\n",
        "# Assuming x_test_reshaped and y_test_one_hot are already loaded and preprocessed.\n",
        "loss, accuracy = model.evaluate(x_test_reshaped, y_test_one_hot)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9849 - loss: 0.0451\n",
            "Test Loss: 0.0338\n",
            "Test Accuracy: 0.9887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show your preprocessing and architecture.**\n"
      ],
      "metadata": {
        "id": "H_g_6qJJdX5Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64c27f88"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dcd682e",
        "outputId": "be753155-e4a4-4aaa-cc87-fc1727177140"
      },
      "source": [
        "# Load the CIFAR-10 dataset\n",
        "(x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train_cifar = x_train_cifar.astype('float32') / 255.0\n",
        "x_test_cifar = x_test_cifar.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train_cifar = tf.keras.utils.to_categorical(y_train_cifar, 10)\n",
        "y_test_cifar = tf.keras.utils.to_categorical(y_test_cifar, 10)\n",
        "\n",
        "print(f\"x_train_cifar shape: {x_train_cifar.shape}\")\n",
        "print(f\"y_train_cifar shape: {y_train_cifar.shape}\")\n",
        "print(f\"x_test_cifar shape: {x_test_cifar.shape}\")\n",
        "print(f\"y_test_cifar shape: {y_test_cifar.shape}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "x_train_cifar shape: (50000, 32, 32, 3)\n",
            "y_train_cifar shape: (50000, 10)\n",
            "x_test_cifar shape: (10000, 32, 32, 3)\n",
            "y_test_cifar shape: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "ab3c393e",
        "outputId": "3adc25de-5617-4ad5-e942-7e33ed88e618"
      },
      "source": [
        "# Define the CNN model architecture for CIFAR-10\n",
        "cifar_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax') # 10 classes for CIFAR-10\n",
        "])\n",
        "\n",
        "# Display the model summary\n",
        "cifar_model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m65,600\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m122,570\u001b[0m (478.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,570</span> (478.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m122,570\u001b[0m (478.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,570</span> (478.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ec6da21"
      },
      "source": [
        "# Compile the CIFAR-10 model\n",
        "cifar_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af21319c",
        "outputId": "928aee21-5618-4421-b47f-a6b8b87891c8"
      },
      "source": [
        "# Train the CIFAR-10 model\n",
        "# Note: Training CIFAR-10 can take longer than MNIST\n",
        "history_cifar = cifar_model.fit(x_train_cifar, y_train_cifar, epochs=10, batch_size=64, validation_split=0.1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 95ms/step - accuracy: 0.3293 - loss: 1.8081 - val_accuracy: 0.4724 - val_loss: 1.4810\n",
            "Epoch 2/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - accuracy: 0.5416 - loss: 1.2794 - val_accuracy: 0.5848 - val_loss: 1.1523\n",
            "Epoch 3/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - accuracy: 0.6151 - loss: 1.0955 - val_accuracy: 0.6212 - val_loss: 1.1102\n",
            "Epoch 4/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 91ms/step - accuracy: 0.6449 - loss: 1.0029 - val_accuracy: 0.6702 - val_loss: 0.9546\n",
            "Epoch 5/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 88ms/step - accuracy: 0.6787 - loss: 0.9092 - val_accuracy: 0.6652 - val_loss: 0.9737\n",
            "Epoch 6/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - accuracy: 0.7024 - loss: 0.8440 - val_accuracy: 0.6930 - val_loss: 0.8819\n",
            "Epoch 7/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - accuracy: 0.7237 - loss: 0.7851 - val_accuracy: 0.7068 - val_loss: 0.8507\n",
            "Epoch 8/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 90ms/step - accuracy: 0.7394 - loss: 0.7415 - val_accuracy: 0.7060 - val_loss: 0.8672\n",
            "Epoch 9/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 91ms/step - accuracy: 0.7596 - loss: 0.6877 - val_accuracy: 0.7258 - val_loss: 0.8045\n",
            "Epoch 10/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 88ms/step - accuracy: 0.7704 - loss: 0.6552 - val_accuracy: 0.7180 - val_loss: 0.8301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7df9790",
        "outputId": "c33bb3fa-4745-43b9-8653-87937d6ec9b2"
      },
      "source": [
        "# Evaluate the CIFAR-10 model\n",
        "loss_cifar, accuracy_cifar = cifar_model.evaluate(x_test_cifar, y_test_cifar, verbose=0)\n",
        "print(f\"CIFAR-10 Test Loss: {loss_cifar:.4f}\")\n",
        "print(f\"CIFAR-10 Test Accuracy: {accuracy_cifar:.4f}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 Test Loss: 0.8615\n",
            "CIFAR-10 Test Accuracy: 0.7050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation.**"
      ],
      "metadata": {
        "id": "SQACVdECgqV9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e702f19",
        "outputId": "d6445efe-889e-4c89-8c86-7c983ea6b617"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82383acd"
      },
      "source": [
        "### 1. Define the CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b74c973e",
        "outputId": "50648f60-a878-4e4b-dee6-c6be7e1c9e59"
      },
      "source": [
        "class MNIST_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # After two MaxPool2d layers (stride 2), the 28x28 image becomes 7x7\n",
        "        # 64 channels * 7 * 7 = 3136 input features to the first dense layer\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10) # 10 classes for MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7) # Flatten the tensor for the fully connected layer\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = MNIST_CNN().to(device)\n",
        "print(model)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST_CNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
            "  (relu3): ReLU()\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47e5ec1c"
      },
      "source": [
        "### 2. Load and Preprocess Data, Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a76f58ae",
        "outputId": "6f9d96c6-5739-4b7a-ea20-2d5ad45bba9b"
      },
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Convert PIL Image to Tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize with MNIST mean and std\n",
        "])\n",
        "\n",
        "# Load MNIST training and test datasets\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 42.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.13MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.4MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.01MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 60000\n",
            "Number of test samples: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12aa0bb3"
      },
      "source": [
        "### 3. Define Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d011aaef"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b6afba5"
      },
      "source": [
        "### 4. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7541bd4f",
        "outputId": "39ab21fd-40bc-41af-b39c-4e8be25779a4"
      },
      "source": [
        "num_epochs = 5\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad() # Clear gradients\n",
        "        loss.backward() # Compute gradients\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch [1/5], Step [100/938], Loss: 0.1306\n",
            "Epoch [1/5], Step [200/938], Loss: 0.1117\n",
            "Epoch [1/5], Step [300/938], Loss: 0.1619\n",
            "Epoch [1/5], Step [400/938], Loss: 0.2401\n",
            "Epoch [1/5], Step [500/938], Loss: 0.0426\n",
            "Epoch [1/5], Step [600/938], Loss: 0.0612\n",
            "Epoch [1/5], Step [700/938], Loss: 0.0625\n",
            "Epoch [1/5], Step [800/938], Loss: 0.0482\n",
            "Epoch [1/5], Step [900/938], Loss: 0.2216\n",
            "Epoch [2/5], Step [100/938], Loss: 0.0455\n",
            "Epoch [2/5], Step [200/938], Loss: 0.1120\n",
            "Epoch [2/5], Step [300/938], Loss: 0.0456\n",
            "Epoch [2/5], Step [400/938], Loss: 0.0109\n",
            "Epoch [2/5], Step [500/938], Loss: 0.0507\n",
            "Epoch [2/5], Step [600/938], Loss: 0.0022\n",
            "Epoch [2/5], Step [700/938], Loss: 0.0184\n",
            "Epoch [2/5], Step [800/938], Loss: 0.0696\n",
            "Epoch [2/5], Step [900/938], Loss: 0.0073\n",
            "Epoch [3/5], Step [100/938], Loss: 0.1190\n",
            "Epoch [3/5], Step [200/938], Loss: 0.0178\n",
            "Epoch [3/5], Step [300/938], Loss: 0.1009\n",
            "Epoch [3/5], Step [400/938], Loss: 0.0050\n",
            "Epoch [3/5], Step [500/938], Loss: 0.0143\n",
            "Epoch [3/5], Step [600/938], Loss: 0.0175\n",
            "Epoch [3/5], Step [700/938], Loss: 0.0132\n",
            "Epoch [3/5], Step [800/938], Loss: 0.1132\n",
            "Epoch [3/5], Step [900/938], Loss: 0.0073\n",
            "Epoch [4/5], Step [100/938], Loss: 0.0039\n",
            "Epoch [4/5], Step [200/938], Loss: 0.0533\n",
            "Epoch [4/5], Step [300/938], Loss: 0.0024\n",
            "Epoch [4/5], Step [400/938], Loss: 0.0128\n",
            "Epoch [4/5], Step [500/938], Loss: 0.0031\n",
            "Epoch [4/5], Step [600/938], Loss: 0.0163\n",
            "Epoch [4/5], Step [700/938], Loss: 0.0031\n",
            "Epoch [4/5], Step [800/938], Loss: 0.0076\n",
            "Epoch [4/5], Step [900/938], Loss: 0.0030\n",
            "Epoch [5/5], Step [100/938], Loss: 0.0012\n",
            "Epoch [5/5], Step [200/938], Loss: 0.0010\n",
            "Epoch [5/5], Step [300/938], Loss: 0.0044\n",
            "Epoch [5/5], Step [400/938], Loss: 0.0502\n",
            "Epoch [5/5], Step [500/938], Loss: 0.0014\n",
            "Epoch [5/5], Step [600/938], Loss: 0.1456\n",
            "Epoch [5/5], Step [700/938], Loss: 0.0406\n",
            "Epoch [5/5], Step [800/938], Loss: 0.0009\n",
            "Epoch [5/5], Step [900/938], Loss: 0.0007\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e314aac9"
      },
      "source": [
        "### 5. Accuracy Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb10bd17",
        "outputId": "1e46f76a-9af8-4a72-f706-8a73c3dc8314"
      },
      "source": [
        "def check_accuracy(loader, model):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculation during evaluation\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1) # Get the index of the max log-probability\n",
        "            num_correct += (predictions == y).sum().item()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    accuracy = (num_correct / num_samples) * 100\n",
        "    print(f'Got {num_correct} / {num_samples} with accuracy {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "print(\"Checking accuracy on training set:\")\n",
        "check_accuracy(train_loader, model)\n",
        "print(\"Checking accuracy on test set:\")\n",
        "check_accuracy(test_loader, model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking accuracy on training set:\n",
            "Got 59788 / 60000 with accuracy 99.65%\n",
            "Checking accuracy on test set:\n",
            "Got 9902 / 10000 with accuracy 99.02%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.02"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 9: Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.**\n"
      ],
      "metadata": {
        "id": "mzdfOexajr4t"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba9fcc53",
        "outputId": "da98710a-8428-413a-9284-9338315a5a6a"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Define base directory for the dummy dataset\n",
        "base_dir = 'custom_image_dataset'\n",
        "\n",
        "# Define class names\n",
        "class_names = ['class_a', 'class_b']\n",
        "\n",
        "# Create dummy dataset directory structure\n",
        "for dataset_type in ['train', 'validation']:\n",
        "    for class_name in class_names:\n",
        "        os.makedirs(os.path.join(base_dir, dataset_type, class_name), exist_ok=True)\n",
        "\n",
        "# Function to create dummy images\n",
        "def create_dummy_image(path, size=(64, 64), color=(0, 0, 0)):\n",
        "    img = Image.new('RGB', size, color)\n",
        "    img.save(path)\n",
        "\n",
        "# Create dummy images for training\n",
        "for i in range(50):\n",
        "    create_dummy_image(os.path.join(base_dir, 'train', 'class_a', f'img_{i}.png'), color=(255, 0, 0)) # Red for class A\n",
        "    create_dummy_image(os.path.join(base_dir, 'train', 'class_b', f'img_{i}.png'), color=(0, 255, 0)) # Green for class B\n",
        "\n",
        "# Create dummy images for validation\n",
        "for i in range(10):\n",
        "    create_dummy_image(os.path.join(base_dir, 'validation', 'class_a', f'val_img_{i}.png'), color=(200, 0, 0))\n",
        "    create_dummy_image(os.path.join(base_dir, 'validation', 'class_b', f'val_img_{i}.png'), color=(0, 200, 0))\n",
        "\n",
        "print(\"Dummy dataset created successfully at: \", os.path.abspath(base_dir))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy dataset created successfully at:  /content/custom_image_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b166d23"
      },
      "source": [
        "### 1. Define Data Generators with Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ba9fbd4",
        "outputId": "076d7ba5-73d7-4058-c839-48b2f2aeab6a"
      },
      "source": [
        "# Define image dimensions and batch size\n",
        "img_height, img_width = 64, 64\n",
        "batch_size = 32\n",
        "\n",
        "# Training Data Generator with Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,             # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,          # Randomly rotate images up to 20 degrees\n",
        "    width_shift_range=0.2,      # Randomly shift images horizontally\n",
        "    height_shift_range=0.2,     # Randomly shift images vertically\n",
        "    shear_range=0.2,            # Apply shear transformation\n",
        "    zoom_range=0.2,             # Apply random zoom\n",
        "    horizontal_flip=True,       # Randomly flip images horizontally\n",
        "    fill_mode='nearest'         # Strategy for filling newly created pixels\n",
        ")\n",
        "\n",
        "# Validation Data Generator (only rescaling)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches from 'train' directory\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(base_dir, 'train'),\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical' # Use 'categorical' for one-hot encoded labels\n",
        ")\n",
        "\n",
        "# Flow validation images in batches from 'validation' directory\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    os.path.join(base_dir, 'validation'),\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "print(f\"Found {train_generator.samples} training images belonging to {train_generator.num_classes} classes.\")\n",
        "print(f\"Found {validation_generator.samples} validation images belonging to {validation_generator.num_classes} classes.\")\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 images belonging to 2 classes.\n",
            "Found 20 images belonging to 2 classes.\n",
            "Found 100 training images belonging to 2 classes.\n",
            "Found 20 validation images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f4b97c2"
      },
      "source": [
        "### 2. Define the CNN Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "85a45e1b",
        "outputId": "0b8dbea2-a365-452f-b147-58f622ec690d"
      },
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(train_generator.num_classes, activation='softmax') # Output layer for number of classes\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m589,952\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,952</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m683,458\u001b[0m (2.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,458</span> (2.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m683,458\u001b[0m (2.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,458</span> (2.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dcc9be9"
      },
      "source": [
        "### 3. Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "617de777"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfda74ec"
      },
      "source": [
        "### 4. Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bb442cb",
        "outputId": "07a262f3-838d-4c0b-812f-28938f609833"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size\n",
        ")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 307ms/step - accuracy: 0.3897 - loss: 0.5909 - val_accuracy: 1.0000 - val_loss: 0.1021\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0802 - val_accuracy: 1.0000 - val_loss: 0.0250\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 0.0084 - val_accuracy: 1.0000 - val_loss: 4.3987e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 3.1590e-06 - val_accuracy: 1.0000 - val_loss: 3.7551e-06\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 1.2277e-07 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 320ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6725c1d9"
      },
      "source": [
        "### 5. Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5808f770",
        "outputId": "e046cff1-5136-4a7a-eb0c-12ae7bf74145"
      },
      "source": [
        "loss, accuracy = model.evaluate(validation_generator, verbose=1)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Optional: Clean up the dummy dataset directory\n",
        "# shutil.rmtree(base_dir)\n",
        "# print(f\"Removed dummy dataset directory: {base_dir}\")\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Test Loss: 0.0000\n",
            "Test Accuracy: 1.0000\n"
          ]
        }
      ]
    }
  ]
}